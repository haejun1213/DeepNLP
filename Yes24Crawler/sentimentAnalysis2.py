import pandas as pd
import numpy as np
import os
import re
from sklearn.utils import resample
from sklearn.model_selection import train_test_split
from sklearn.utils.class_weight import compute_class_weight
from konlpy.tag import Okt
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.optimizers import RMSprop
import joblib

# ----------------------------
# ë¶ˆìš©ì–´ & í˜•íƒœì†Œ ë¶„ì„ ì¤€ë¹„
# ----------------------------
stopwords = set([
    'ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ë“¤', 'ì˜', 'ê°€', 'ì€', 'ëŠ”', 'ì—', 'ì™€', 'í•œ', 'í•˜ë‹¤', 'ì„', 'ë¥¼',
    'ìœ¼ë¡œ', 'ë„', 'ìœ¼ë¡œì„œ', 'ìœ¼ë¡œì¨', 'ê·¸ë¦¬ê³ ', 'ê·¸ëŸ¬ë‚˜', 'í•˜ì§€ë§Œ', 'ë•Œë¬¸ì—', 'ë”', 'ì—ì„œ', 'í•´ì„œ'
])
okt = Okt()

def preprocess_text(text):
    text = re.sub(r"[^ê°€-í£a-zA-Z0-9\s]", "", text)
    tokens = okt.pos(text, stem=True)
    filtered = [word for word, pos in tokens if pos in ['Noun', 'Verb', 'Adjective'] and word not in stopwords]
    return ' '.join(filtered)

# ----------------------------
# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ë° ì „ì²˜ë¦¬
# ----------------------------
review_df = pd.read_csv('C:/Lec/_DeepNLP25/Crawler/Yes24Crawler/yes24_reviews_dataset_balanced8.csv')
review_df = review_df.dropna(subset=['review'])

# í´ë˜ìŠ¤ ê· í˜• ë§ì¶”ê¸°
pos_df = review_df[review_df['sentiment'] == 'positive']
neu_df = review_df[review_df['sentiment'] == 'neutral']
neg_df = review_df[review_df['sentiment'] == 'negative']
min_count = min(len(pos_df), len(neu_df), len(neg_df))
pos_df = resample(pos_df, replace=False, n_samples=min_count, random_state=42)
neu_df = resample(neu_df, replace=False, n_samples=min_count, random_state=42)
neg_df = resample(neg_df, replace=False, n_samples=min_count, random_state=42)
balanced_df = pd.concat([pos_df, neu_df, neg_df]).sample(frac=1).reset_index(drop=True)

# ë¦¬ë·° ì •ì œ
print(" ë¦¬ë·° ì „ì²˜ë¦¬ ì¤‘...")
balanced_df['clean_review'] = balanced_df['review'].apply(preprocess_text)

# ë¼ë²¨ ì¸ì½”ë”©
label_map = {'negative': 0, 'neutral': 1, 'positive': 2}
balanced_df['label'] = balanced_df['sentiment'].map(label_map)
input_list = list(balanced_df['clean_review'])
label_list = list(balanced_df['label'])

# í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
X_train_text, X_test_text, y_train, y_test = train_test_split(
    input_list, label_list, test_size=0.1, stratify=label_list, random_state=42)

# ----------------------------
# í† í¬ë‚˜ì´ì € ë° ì‹œí€€ì‹±
# ----------------------------
vocab_size = 25000
tokenizer = Tokenizer(num_words=vocab_size + 1, oov_token="<OOV>")
tokenizer.fit_on_texts(input_list)

train_sequences = tokenizer.texts_to_sequences(X_train_text)
test_sequences = tokenizer.texts_to_sequences(X_test_text)

# ë¹ˆ ì‹œí€€ìŠ¤ ì œê±°
filtered_train = [(seq, label) for seq, label in zip(train_sequences, y_train) if len(seq) > 0]
filtered_test = [(seq, label) for seq, label in zip(test_sequences, y_test) if len(seq) > 0]
train_sequences, y_train = zip(*filtered_train)
test_sequences, y_test = zip(*filtered_test)
train_sequences = list(train_sequences)
test_sequences = list(test_sequences)
y_train = list(y_train)
y_test = list(y_test)

# íŒ¨ë”©
max_len = 50
X_train = pad_sequences(train_sequences, maxlen=max_len, padding='post', truncating='post')
X_test = pad_sequences(test_sequences, maxlen=max_len, padding='post', truncating='post')

# ì›-í•« ì¸ì½”ë”©
y_train = to_categorical(y_train, num_classes=3)
y_test = to_categorical(y_test, num_classes=3)

# ----------------------------
# í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°
# ----------------------------
class_weights = compute_class_weight(class_weight='balanced',
                                     classes=np.unique(label_list),
                                     y=label_list)
class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}
print("ğŸ“Š í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜:", class_weight_dict)

# ----------------------------
# ëª¨ë¸ ì •ì˜
# ----------------------------
model = Sequential([
    Embedding(input_dim=vocab_size + 1, output_dim=64, input_length=max_len),
    Bidirectional(LSTM(64, return_sequences=False)),
    Dense(64, activation='tanh'),
    Dense(3, activation='softmax')
])
model.compile(optimizer=RMSprop(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# ----------------------------
# ì½œë°± ë° í•™ìŠµ
# ----------------------------
os.makedirs('./model', exist_ok=True)
checkpoint_path = './model/best_model_lstm_okt_imp.h5'
es = EarlyStopping(monitor='val_accuracy', mode='max', patience=3, verbose=1)
mc = ModelCheckpoint(checkpoint_path, monitor='val_accuracy', mode='max', save_best_only=True, verbose=1)

history = model.fit(X_train, y_train,
                    epochs=20,
                    batch_size=128,
                    validation_split=0.1,
                    callbacks=[es, mc],
                    class_weight=class_weight_dict)

# ----------------------------
# í‰ê°€ ë° ì €ì¥
# ----------------------------
model.load_weights(checkpoint_path)
loss, acc = model.evaluate(X_test, y_test)
print(f"\nğŸ§ª ìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„: {acc * 100:.2f}%")

joblib.dump(tokenizer, "./model/yes24_tokenizer.pkl")
