{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 빈도수 기반 텍스트 표현 (BoW, TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW 기반의 카운트 벡터 생성\n",
    "\n",
    "* 사이킷런의 CountVectorizer 클래스 활용 연습\n",
    "    * sklearn 설치 : **pip install scikit-learn**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 영어 문서 카운트 벡터 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizer 객체 생성\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cVectorizer = CountVectorizer()\n",
    "\n",
    "\n",
    "# 특징 집합 생성\n",
    "cVectorizer.fit(sample_corpus)\n",
    "print(cVectorizer.get_feature_names_out())\n",
    "\n",
    "# 카운트 벡터 생성\n",
    "sample_dtm = cVectorizer.transform(sample_corpus)\n",
    "\n",
    "# DTM 출력\n",
    "print(sample_dtm.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 참고\n",
    "feature_set = cVectorizer.vocabulary_\n",
    "print(feature_set)\n",
    "\n",
    "# index 0부터 출력\n",
    "print(sorted(feature_set.items(), key=lambda x : x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한글 문서 카운트 벡터 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_corpus = [\n",
    "    '자연어처리 강의를 시작하겠습니다.',\n",
    "    '자연어처리는 재미있습니다.',\n",
    "    '밥을 먹고 강의를 듣고 있습니다.',\n",
    "    '이번 자연어처리 강의는 한국어 자연어처리입니다.' \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#한국어 토크나이저 정의\n",
    "from konlpy.tag import Okt\n",
    "def my_tokenizer(text):\n",
    "    t = Okt()\n",
    "    return t.nouns(text)\n",
    "\n",
    "my_stopwords = ['이번']\n",
    "\n",
    "# Vectorizer 객체 생성\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# cVectorizer = CountVectorizer(tokenizer=t.nouns)\n",
    "cVectorizer = CountVectorizer(tokenizer=my_tokenizer, stop_words=my_stopwords)\n",
    "\n",
    "# 특징 집합 생성\n",
    "cVectorizer.fit(sample_corpus)\n",
    "print(cVectorizer.get_feature_names_out())\n",
    "print(cVectorizer.vocabulary_)\n",
    "\n",
    "# 카운트 벡터 생성\n",
    "sample_dtm = cVectorizer.transform(sample_corpus)\n",
    "\n",
    "# DTM 출력\n",
    "print(sample_dtm.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizer에 사용할 한국어 tokenizer 정의 (정제, 불용어제거 등의 기능 추가)\n",
    "def my_tokenizer(text):\n",
    "    t = Okt()\n",
    "    my_tags = ['Noun', 'Verd', 'Adjective']\n",
    "    tokens = [word for word, tag in t.pos(text) if tag in my_tags]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자 정의 tokenizer를 사용하여 Vectorizer 객체 생성\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cVectorizer = CountVectorizer(tokenizer=my_tokenizer)\n",
    "\n",
    "# 특징 집합 생성\n",
    "cVectorizer.fit(sample_corpus)\n",
    "print(cVectorizer.get_feature_names_out())\n",
    "\n",
    "# 카운트 벡터 생성\n",
    "sample_dtm = cVectorizer.transform(sample_corpus)\n",
    "\n",
    "# DTM 출력\n",
    "print(sample_dtm.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF 벡터 생성\n",
    "* sklearn.feature_extraction.text.TfidfVectorizer 사용\n",
    "1. 한글 토크나이저 정의\n",
    "2. 특징 추출 모델 생성 : Vectorizer -> fit()\n",
    "3. 문서별 특징 벡터 추출 : transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한글 토크나이저 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "def my_tokenizer(text):\n",
    "    t = Okt()\n",
    "    my_tags = ['Noun','Verb','Adjective']\n",
    "    tokens = [word for word, tag in t.pos(text) if tag in my_tags]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정의한 tokenizer로 1~2문서 확인\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 특징 추출 모델 생성 : TfIdfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfIdfVectorizer 객체 생성 (tokenizer 지정, 최대 단어 수지 지정)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tVectorizer = TfidfVectorizer(tokenizer=my_tokenizer, max_features=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문서별 특징 벡터 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특징 집합과 특징 벡터 계산을 위한 데이터 추출\n",
    "tVectorizer.fit(sample_corpus)\n",
    "print(tVectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특징 벡터 추출\n",
    "sample_tfidf_dtm = tVectorizer.transform(sample_corpus)\n",
    "print(sample_tfidf_dtm.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다음(Daum) 영화 리뷰 TF-IDF 벡터 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv 파일 → DataFrame으로 업로드\n",
    "import pandas as pd\n",
    "movie_df = pd.read_csv('./data/daum_movie_review.csv')\n",
    "movie_df.head()\n",
    "review_corpus = movie_df['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Vectorizer 객체 생성\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "review_tv = TfidfVectorizer(tokenizer=my_tokenizer, max_features=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizer 모델 생성\n",
    "review_tv.fit(review_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 특징 집합 구성 단어 확인\n",
    "review_tv.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 새로운 텍스트의 tfidf 특징 벡터 추출\n",
    "print(review_tv.transform([\"이 영화 굿~~~~\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "**[참고] Tfidf 특징 벡터 추출 모델을 파일로 저장하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(review_tv, './model/daum_review_tv.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_tv = joblib.load('./model/daum_review_tv.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loaded_tv.transform([\"이 영화 굿~~~~\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepNLPEnv",
   "language": "python",
   "name": "deepnlpenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
